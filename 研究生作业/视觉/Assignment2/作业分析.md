# CS231n Assignment 2 ä½œä¸šåˆ†æ

## ğŸ“‹ ä½œä¸šæ¦‚è¿°

è¿™æ˜¯CS231nè¯¾ç¨‹çš„Assignment 2ï¼Œä¸»è¦ç›®æ ‡æ˜¯å®ç°å’Œè®­ç»ƒå‡ ç§ç»å…¸çš„å›¾åƒåˆ†ç±»ç®—æ³•ï¼Œå¹¶åœ¨CIFAR-10æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚ä½œä¸šåŒ…å«5ä¸ªä¸»è¦éƒ¨åˆ†ï¼š

1. **k-Nearest Neighbor (kNN)** - å®ç°kè¿‘é‚»åˆ†ç±»å™¨
2. **Support Vector Machine (SVM)** - å®ç°å¤šç±»SVMåˆ†ç±»å™¨
3. **Softmax** - å®ç°Softmaxåˆ†ç±»å™¨
4. **Two-layer Neural Network** - å®ç°ä¸¤å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œ
5. **Image Features** - ä½¿ç”¨å›¾åƒç‰¹å¾æå‡åˆ†ç±»æ€§èƒ½

## ğŸ¯ æ ¸å¿ƒç›®æ ‡

- ç†è§£å›¾åƒåˆ†ç±»çš„åŸºæœ¬æµç¨‹
- æŒæ¡å‘é‡åŒ–ç¼–ç¨‹ï¼ˆé¿å…ä½¿ç”¨æ˜¾å¼å¾ªç¯ï¼‰
- ç†è§£æŸå¤±å‡½æ•°å’Œæ¢¯åº¦è®¡ç®—
- æŒæ¡è¶…å‚æ•°è°ƒä¼˜å’Œäº¤å‰éªŒè¯
- ç†è§£ç¥ç»ç½‘ç»œçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­

---

## ğŸ“ ç¬¬ä¸€éƒ¨åˆ†ï¼šk-Nearest Neighbor (kNN)

### ä»»åŠ¡è¦æ±‚

å®ç°kNNåˆ†ç±»å™¨ï¼ŒåŒ…æ‹¬ï¼š
- è®¡ç®—è·ç¦»ï¼ˆä¸‰ç§å®ç°æ–¹å¼ï¼šåŒå¾ªç¯ã€å•å¾ªç¯ã€æ— å¾ªç¯ï¼‰
- é¢„æµ‹æ ‡ç­¾ï¼ˆæ ¹æ®kä¸ªæœ€è¿‘é‚»æŠ•ç¥¨ï¼‰

### éœ€è¦å®ç°çš„å‡½æ•°

**æ–‡ä»¶ï¼š`cs231n/classifiers/k_nearest_neighbor.py`**

1. **`compute_distances_two_loops(self, X)`**
   - ä½¿ç”¨åŒå¾ªç¯è®¡ç®—L2è·ç¦»
   - å…¬å¼ï¼š`dists[i, j] = sqrt(sum((X[i] - X_train[j])^2))`
   - æ³¨æ„ï¼šä¸èƒ½ä½¿ç”¨`np.linalg.norm()`

2. **`compute_distances_one_loop(self, X)`**
   - ä½¿ç”¨å•å¾ªç¯è®¡ç®—è·ç¦»
   - å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œå‘é‡åŒ–è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»

3. **`compute_distances_no_loops(self, X)`**
   - å®Œå…¨å‘é‡åŒ–å®ç°
   - æç¤ºï¼šä½¿ç”¨çŸ©é˜µä¹˜æ³•å’Œå¹¿æ’­
   - å…¬å¼å±•å¼€ï¼š`(a-b)^2 = a^2 - 2ab + b^2`
   - ä½¿ç”¨ï¼š`np.dot()`, `np.sum()`, å¹¿æ’­æœºåˆ¶

4. **`predict_labels(self, dists, k=1)`**
   - æ ¹æ®è·ç¦»çŸ©é˜µé¢„æµ‹æ ‡ç­¾
   - æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»
   - æŠ•ç¥¨å†³å®šæœ€ç»ˆæ ‡ç­¾ï¼ˆå¹³å±€æ—¶é€‰æ‹©è¾ƒå°çš„æ ‡ç­¾ï¼‰

### å®ç°æ€è·¯

**è·ç¦»è®¡ç®—ï¼ˆæ— å¾ªç¯ç‰ˆæœ¬ï¼‰ï¼š**
```python
# å±•å¼€L2è·ç¦»å…¬å¼ï¼š(a-b)^2 = a^2 - 2ab + b^2
# dists[i, j] = sqrt(sum((X[i] - X_train[j])^2))
# å‘é‡åŒ–ï¼š
# - X^2: (num_test, D) -> sum -> (num_test, 1)
# - X_train^2: (num_train, D) -> sum -> (num_train,)
# - 2*X*X_train^T: (num_test, num_train)
dists = np.sqrt(
    np.sum(X**2, axis=1, keepdims=True) + 
    np.sum(self.X_train**2, axis=1) - 
    2 * np.dot(X, self.X_train.T)
)
```

**æ ‡ç­¾é¢„æµ‹ï¼š**
```python
# 1. æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»çš„ç´¢å¼•
closest_indices = np.argsort(dists[i])[:k]
# 2. è·å–è¿™äº›é‚»å±…çš„æ ‡ç­¾
closest_y = self.y_train[closest_indices]
# 3. æŠ•ç¥¨ï¼ˆä½¿ç”¨np.bincountæˆ–Counterï¼‰
y_pred[i] = np.argmax(np.bincount(closest_y))
```

### æ³¨æ„äº‹é¡¹

- æ³¨æ„æ•°å€¼ç¨³å®šæ€§ï¼ˆé¿å…å¼€æ–¹åçš„è´Ÿæ•°ï¼‰
- ä½¿ç”¨`np.argsort()`æ‰¾æœ€è¿‘é‚»
- kçš„é€‰æ‹©é€šè¿‡äº¤å‰éªŒè¯ç¡®å®š

---

## ğŸ“ ç¬¬äºŒéƒ¨åˆ†ï¼šSupport Vector Machine (SVM)

### ä»»åŠ¡è¦æ±‚

å®ç°å¤šç±»SVMåˆ†ç±»å™¨ï¼ŒåŒ…æ‹¬ï¼š
- æŸå¤±å‡½æ•°ï¼ˆnaiveå’Œvectorizedç‰ˆæœ¬ï¼‰
- æ¢¯åº¦è®¡ç®—ï¼ˆnaiveå’Œvectorizedç‰ˆæœ¬ï¼‰
- ä½¿ç”¨SGDä¼˜åŒ–
- è¶…å‚æ•°è°ƒä¼˜

### éœ€è¦å®ç°çš„å‡½æ•°

**æ–‡ä»¶ï¼š`cs231n/classifiers/linear_svm.py`**

1. **`svm_loss_naive(W, X, y, reg)`**
   - ä½¿ç”¨å¾ªç¯å®ç°SVMæŸå¤±å’Œæ¢¯åº¦
   - æŸå¤±å…¬å¼ï¼š`L_i = sum(max(0, s_j - s_yi + 1))` for j != y_i
   - æ¢¯åº¦ï¼šå¯¹æ¯ä¸ªæ ·æœ¬ï¼Œå¦‚æœmargin > 0ï¼Œåˆ™æ›´æ–°æ¢¯åº¦

2. **`svm_loss_vectorized(W, X, y, reg)`**
   - å®Œå…¨å‘é‡åŒ–å®ç°
   - å…³é”®ï¼šä½¿ç”¨å¹¿æ’­å’ŒçŸ©é˜µè¿ç®—é¿å…å¾ªç¯

### å®ç°æ€è·¯

**å‘é‡åŒ–æŸå¤±è®¡ç®—ï¼š**
```python
# 1. è®¡ç®—æ‰€æœ‰åˆ†æ•°
scores = X.dot(W)  # (N, C)
# 2. è·å–æ­£ç¡®ç±»åˆ«çš„åˆ†æ•°
correct_scores = scores[np.arange(N), y].reshape(-1, 1)  # (N, 1)
# 3. è®¡ç®—margin
margins = np.maximum(0, scores - correct_scores + 1)  # (N, C)
# 4. å°†æ­£ç¡®ç±»åˆ«çš„marginè®¾ä¸º0
margins[np.arange(N), y] = 0
# 5. è®¡ç®—æŸå¤±
loss = np.sum(margins) / N + reg * np.sum(W * W)
```

**å‘é‡åŒ–æ¢¯åº¦è®¡ç®—ï¼š**
```python
# 1. åˆ›å»ºæ¢¯åº¦çŸ©é˜µ
dW = np.zeros_like(W)
# 2. è®¡ç®—margin > 0çš„mask
mask = margins > 0  # (N, C)
# 3. å¯¹æ¯ä¸ªç±»åˆ«ï¼Œè®¡ç®—æ¢¯åº¦
mask[np.arange(N), y] = -np.sum(mask, axis=1)  # æ­£ç¡®ç±»åˆ«ç‰¹æ®Šå¤„ç†
dW = X.T.dot(mask) / N + 2 * reg * W
```

### æ³¨æ„äº‹é¡¹

- æ³¨æ„æ­£ç¡®ç±»åˆ«çš„å¤„ç†ï¼ˆmarginåº”è¯¥ä¸º0ï¼‰
- æ­£åˆ™åŒ–é¡¹ï¼š`reg * sum(W^2)`
- æ¢¯åº¦æ£€æŸ¥ï¼šä½¿ç”¨æ•°å€¼æ¢¯åº¦éªŒè¯å®ç°æ­£ç¡®æ€§

---

## ğŸ“ ç¬¬ä¸‰éƒ¨åˆ†ï¼šSoftmax

### ä»»åŠ¡è¦æ±‚

å®ç°Softmaxåˆ†ç±»å™¨ï¼ŒåŒ…æ‹¬ï¼š
- æŸå¤±å‡½æ•°ï¼ˆnaiveå’Œvectorizedç‰ˆæœ¬ï¼‰
- æ¢¯åº¦è®¡ç®—
- æ•°å€¼ç¨³å®šæ€§å¤„ç†

### éœ€è¦å®ç°çš„å‡½æ•°

**æ–‡ä»¶ï¼š`cs231n/classifiers/softmax.py`**

1. **`softmax_loss_naive(W, X, y, reg)`**
   - ä½¿ç”¨å¾ªç¯å®ç°SoftmaxæŸå¤±å’Œæ¢¯åº¦
   - æ³¨æ„æ•°å€¼ç¨³å®šæ€§ï¼šå‡å»æœ€å¤§å€¼

2. **`softmax_loss_vectorized(W, X, y, reg)`**
   - å®Œå…¨å‘é‡åŒ–å®ç°

### å®ç°æ€è·¯

**SoftmaxæŸå¤±ï¼ˆæ•°å€¼ç¨³å®šï¼‰ï¼š**
```python
# 1. è®¡ç®—åˆ†æ•°
scores = X.dot(W)  # (N, C)
# 2. æ•°å€¼ç¨³å®šæ€§ï¼šå‡å»æ¯è¡Œçš„æœ€å¤§å€¼
scores -= np.max(scores, axis=1, keepdims=True)
# 3. è®¡ç®—exp
exp_scores = np.exp(scores)
# 4. è®¡ç®—æ¦‚ç‡
probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
# 5. è®¡ç®—æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
loss = -np.sum(np.log(probs[np.arange(N), y])) / N
loss += reg * np.sum(W * W)
```

**æ¢¯åº¦è®¡ç®—ï¼š**
```python
# 1. åˆ›å»ºæ¢¯åº¦çŸ©é˜µ
dW = np.zeros_like(W)
# 2. å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œæ¦‚ç‡å‡1ï¼ˆæ­£ç¡®ç±»åˆ«ï¼‰
dscores = probs.copy()
dscores[np.arange(N), y] -= 1
# 3. è®¡ç®—æ¢¯åº¦
dW = X.T.dot(dscores) / N + 2 * reg * W
```

### æ³¨æ„äº‹é¡¹

- **æ•°å€¼ç¨³å®šæ€§è‡³å…³é‡è¦**ï¼šå¿…é¡»å‡å»æœ€å¤§å€¼ï¼Œå¦åˆ™expä¼šæº¢å‡º
- ä½¿ç”¨`np.log()`æ—¶æ³¨æ„é¿å…log(0)

---

## ğŸ“ ç¬¬å››éƒ¨åˆ†ï¼šTwo-layer Neural Network

### ä»»åŠ¡è¦æ±‚

å®ç°ä¸¤å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬ï¼š
- å‰å‘ä¼ æ’­
- åå‘ä¼ æ’­
- ä½¿ç”¨SGDè®­ç»ƒ

### ç½‘ç»œæ¶æ„

```
input (N, D) 
  -> fully connected (D, H) 
  -> ReLU 
  -> fully connected (H, C) 
  -> softmax
```

### éœ€è¦å®ç°çš„å‡½æ•°

**æ–‡ä»¶ï¼š`cs231n/classifiers/neural_net.py`**

1. **`loss(self, X, y=None, reg=0.0)`**
   - å‰å‘ä¼ æ’­ï¼šè®¡ç®—åˆ†æ•°
   - è®¡ç®—æŸå¤±ï¼ˆSoftmaxæŸå¤± + L2æ­£åˆ™åŒ–ï¼‰
   - åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦

### å®ç°æ€è·¯

**å‰å‘ä¼ æ’­ï¼š**
```python
# ç¬¬ä¸€å±‚
z1 = X.dot(W1) + b1  # (N, H)
a1 = np.maximum(0, z1)  # ReLU: (N, H)
# ç¬¬äºŒå±‚
scores = a1.dot(W2) + b2  # (N, C)
```

**æŸå¤±è®¡ç®—ï¼š**
```python
# ä½¿ç”¨SoftmaxæŸå¤±ï¼ˆæ•°å€¼ç¨³å®šï¼‰
exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))
probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
loss = -np.sum(np.log(probs[np.arange(N), y])) / N
loss += reg * (np.sum(W1 * W1) + np.sum(W2 * W2))
```

**åå‘ä¼ æ’­ï¼š**
```python
# 1. Softmaxå±‚æ¢¯åº¦
dscores = probs.copy()
dscores[np.arange(N), y] -= 1
dscores /= N

# 2. ç¬¬äºŒå±‚æ¢¯åº¦
dW2 = a1.T.dot(dscores) + 2 * reg * W2
db2 = np.sum(dscores, axis=0)

# 3. ç¬¬ä¸€å±‚æ¢¯åº¦ï¼ˆé€šè¿‡ReLUï¼‰
da1 = dscores.dot(W2.T)
dz1 = da1.copy()
dz1[z1 <= 0] = 0  # ReLUæ¢¯åº¦

# 4. ç¬¬ä¸€å±‚æƒé‡æ¢¯åº¦
dW1 = X.T.dot(dz1) + 2 * reg * W1
db1 = np.sum(dz1, axis=0)
```

### æ³¨æ„äº‹é¡¹

- ReLUçš„æ¢¯åº¦ï¼šè¾“å…¥>0æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0
- æ³¨æ„çŸ©é˜µç»´åº¦åŒ¹é…
- æ­£åˆ™åŒ–åªåº”ç”¨äºæƒé‡ï¼Œä¸åº”ç”¨äºåç½®

---

## ğŸ“ ç¬¬äº”éƒ¨åˆ†ï¼šImage Features

### ä»»åŠ¡è¦æ±‚

ä½¿ç”¨å›¾åƒç‰¹å¾ï¼ˆè€ŒéåŸå§‹åƒç´ ï¼‰æå‡åˆ†ç±»æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š
- é¢œè‰²ç›´æ–¹å›¾ç‰¹å¾
- HOGç‰¹å¾
- ç‰¹å¾æå–å’Œç»„åˆ

### éœ€è¦å®ç°çš„å‡½æ•°

**æ–‡ä»¶ï¼š`cs231n/features.py`**

å¤§éƒ¨åˆ†å‡½æ•°å·²å®ç°ï¼Œä¸»è¦æ˜¯åœ¨notebookä¸­ï¼š
- æå–ç‰¹å¾
- è®­ç»ƒåˆ†ç±»å™¨
- æ¯”è¾ƒæ€§èƒ½

### å®ç°æ€è·¯

**ç‰¹å¾æå–æµç¨‹ï¼š**
```python
# 1. æå–é¢œè‰²ç›´æ–¹å›¾
color_hist = color_histogram_hsv(img)
# 2. æå–HOGç‰¹å¾
hog = hog_feature(img)
# 3. ç»„åˆç‰¹å¾
features = np.hstack([color_hist, hog])
```

**è®­ç»ƒæµç¨‹ï¼š**
```python
# 1. æå–æ‰€æœ‰å›¾åƒçš„ç‰¹å¾
X_train_feats = extract_features(X_train, [color_histogram_hsv, hog_feature])
X_val_feats = extract_features(X_val, [color_histogram_hsv, hog_feature])
X_test_feats = extract_features(X_test, [color_histogram_hsv, hog_feature])

# 2. å½’ä¸€åŒ–
mean = np.mean(X_train_feats, axis=0)
X_train_feats -= mean
X_val_feats -= mean
X_test_feats -= mean

# 3. è®­ç»ƒåˆ†ç±»å™¨ï¼ˆSVMæˆ–Softmaxï¼‰
# 4. è¯„ä¼°æ€§èƒ½
```

---

## ğŸš€ å®Œæˆæ­¥éª¤

### 1. ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ä¸‹è½½CIFAR-10æ•°æ®é›†
cd cs231n/datasets
./get_datasets.sh
```

### 2. å®ç°é¡ºåºå»ºè®®

1. **kNN** - æœ€ç®€å•ï¼Œå…ˆå®Œæˆ
   - å…ˆå®ç°åŒå¾ªç¯ç‰ˆæœ¬ï¼ˆç†è§£ç®—æ³•ï¼‰
   - å†å®ç°å•å¾ªç¯ç‰ˆæœ¬ï¼ˆç†è§£å‘é‡åŒ–ï¼‰
   - æœ€åå®ç°æ— å¾ªç¯ç‰ˆæœ¬ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰

2. **SVM** - ç†è§£æŸå¤±å‡½æ•°å’Œæ¢¯åº¦
   - å…ˆå®ç°naiveç‰ˆæœ¬ï¼ˆç†è§£ç®—æ³•ï¼‰
   - å†å®ç°vectorizedç‰ˆæœ¬ï¼ˆæé«˜æ•ˆç‡ï¼‰
   - è¿›è¡Œæ¢¯åº¦æ£€æŸ¥

3. **Softmax** - ç±»ä¼¼SVMä½†æ›´å¤æ‚
   - æ³¨æ„æ•°å€¼ç¨³å®šæ€§
   - å®ç°naiveå’Œvectorizedç‰ˆæœ¬

4. **Neural Network** - æœ€å¤æ‚
   - å…ˆç†è§£å‰å‘ä¼ æ’­
   - å†å®ç°åå‘ä¼ æ’­
   - æ³¨æ„æ¢¯åº¦æ£€æŸ¥

5. **Image Features** - åº”ç”¨å‰é¢å­¦åˆ°çš„çŸ¥è¯†
   - æå–ç‰¹å¾
   - è®­ç»ƒåˆ†ç±»å™¨
   - æ¯”è¾ƒæ€§èƒ½

### 3. è°ƒè¯•æŠ€å·§

- **æ¢¯åº¦æ£€æŸ¥**ï¼šä½¿ç”¨æ•°å€¼æ¢¯åº¦éªŒè¯è§£ææ¢¯åº¦
- **å°æ•°æ®é›†æµ‹è¯•**ï¼šå…ˆç”¨toy dataæµ‹è¯•
- **æ‰“å°ä¸­é—´ç»“æœ**ï¼šæ£€æŸ¥ç»´åº¦ã€æ•°å€¼èŒƒå›´
- **å¯è§†åŒ–**ï¼šç»˜åˆ¶æŸå¤±æ›²çº¿ã€æƒé‡å¯è§†åŒ–

### 4. æ€§èƒ½ä¼˜åŒ–

- **å‘é‡åŒ–**ï¼šé¿å…æ˜¾å¼å¾ªç¯
- **å¹¿æ’­æœºåˆ¶**ï¼šåˆ©ç”¨numpyå¹¿æ’­
- **å†…å­˜ç®¡ç†**ï¼šæ³¨æ„å¤§æ•°ç»„çš„å†…å­˜ä½¿ç”¨

---

## ğŸ“Š é¢„æœŸç»“æœ

- **kNN**: å‡†ç¡®ç‡çº¦27-28%ï¼ˆk=10ï¼‰
- **SVM**: å‡†ç¡®ç‡çº¦37-39%
- **Softmax**: å‡†ç¡®ç‡çº¦36-38%
- **Neural Network**: å‡†ç¡®ç‡çº¦48-52%ï¼ˆå–å†³äºè¶…å‚æ•°ï¼‰
- **Image Features**: å‡†ç¡®ç‡çº¦55-60%

---

## âš ï¸ å¸¸è§é—®é¢˜

1. **ç»´åº¦ä¸åŒ¹é…**
   - æ£€æŸ¥çŸ©é˜µä¹˜æ³•çš„ç»´åº¦
   - æ³¨æ„keepdimså‚æ•°

2. **æ•°å€¼ä¸ç¨³å®š**
   - Softmaxå¿…é¡»å‡å»æœ€å¤§å€¼
   - æ³¨æ„log(0)çš„æƒ…å†µ

3. **æ¢¯åº¦é”™è¯¯**
   - ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥éªŒè¯
   - æ³¨æ„æ­£åˆ™åŒ–é¡¹çš„æ¢¯åº¦

4. **æ€§èƒ½é—®é¢˜**
   - ç¡®ä¿ä½¿ç”¨å‘é‡åŒ–å®ç°
   - é¿å…ä¸å¿…è¦çš„å¤åˆ¶

---

## ğŸ“š å‚è€ƒèµ„æ–™

- CS231nè¯¾ç¨‹ç½‘ç«™ï¼šhttp://cs231n.github.io/
- Assignment 2é¡µé¢ï¼šhttp://cs231n.github.io/assignments2019/assignment2/
- NumPyæ–‡æ¡£ï¼šhttps://numpy.org/doc/

---

## âœ… æ£€æŸ¥æ¸…å•

- [ ] kNNä¸‰ä¸ªè·ç¦»è®¡ç®—å‡½æ•°
- [ ] kNNé¢„æµ‹å‡½æ•°
- [ ] SVMæŸå¤±å‡½æ•°ï¼ˆnaiveå’Œvectorizedï¼‰
- [ ] SVMæ¢¯åº¦ï¼ˆnaiveå’Œvectorizedï¼‰
- [ ] SoftmaxæŸå¤±å‡½æ•°ï¼ˆnaiveå’Œvectorizedï¼‰
- [ ] Softmaxæ¢¯åº¦ï¼ˆnaiveå’Œvectorizedï¼‰
- [ ] ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­
- [ ] ç¥ç»ç½‘ç»œåå‘ä¼ æ’­
- [ ] æ‰€æœ‰æ¢¯åº¦æ£€æŸ¥é€šè¿‡
- [ ] è¶…å‚æ•°è°ƒä¼˜å®Œæˆ
- [ ] ç‰¹å¾æå–å’Œåˆ†ç±»å®Œæˆ
