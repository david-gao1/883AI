# k-最近邻（kNN）分类器实验报告

## 摘要

本实验实现了k-最近邻（k-Nearest Neighbor, kNN）分类器，并在CIFAR-10数据集上进行了图像分类任务。实验重点完成了三种不同效率的距离计算方法（双重循环、单循环、完全向量化），并通过5折交叉验证确定了最优的k值。实验结果表明，完全向量化的实现相比双重循环版本速度提升了约116倍，最终在测试集上达到了28.2%的分类准确率（k=10）。实验验证了向量化操作在NumPy中的重要性，以及交叉验证在超参数选择中的关键作用。

**关键词**：k-最近邻、图像分类、CIFAR-10、向量化、交叉验证、L2距离

## 1 实验目的

本实验旨在实现一个完整的k-最近邻分类器，重点完成以下目标：

1. **理解kNN算法的基本原理**：通过记忆训练数据，在测试时基于距离进行预测
2. **掌握向量化编程**：实现三种不同效率的距离计算方法，理解NumPy向量化操作的重要性
3. **学习交叉验证**：使用5折交叉验证选择最优超参数k
4. **熟悉图像分类流程**：在CIFAR-10数据集上完成端到端的分类任务

## 2 kNN算法实现

### 2.1 算法核心思想

kNN分类器是一种基于实例的学习方法（Instance-based Learning），其核心思想是：

- **训练阶段**：简单记忆所有训练数据和标签
- **预测阶段**：对于每个测试样本，找到训练集中距离最近的k个样本，通过投票决定预测标签

**距离度量**：本实验使用L2距离（欧氏距离）：
$$d(x_i, x_j) = \sqrt{\sum_{d=1}^{D}(x_{i,d} - x_{j,d})^2}$$

### 2.2 距离计算的三种实现

#### 2.2.1 双重循环实现（compute_distances_two_loops）

**实现代码**：
```python
def compute_distances_two_loops(self, X):
    num_test = X.shape[0]
    num_train = self.X_train.shape[0]
    dists = np.zeros((num_test, num_train))
    for i in range(num_test):
        for j in range(num_train):
            dists[i, j] = np.sqrt(np.sum((X[i] - self.X_train[j]) ** 2))
    return dists
```

**原理分析**：
- 使用双重循环遍历所有测试样本和训练样本的组合
- 对每一对样本计算L2距离：$\sqrt{\sum_{d}(x_{test,d} - x_{train,d})^2}$
- 时间复杂度：$O(N_{test} \times N_{train} \times D)$，其中D是特征维度

**特点**：
- 实现简单直观，易于理解
- 效率最低，适合小规模数据或理解算法原理

#### 2.2.2 单循环实现（compute_distances_one_loop）

**实现代码**：
```python
def compute_distances_one_loop(self, X):
    num_test = X.shape[0]
    num_train = self.X_train.shape[0]
    dists = np.zeros((num_test, num_train))
    for i in range(num_test):
        dists[i, :] = np.sqrt(np.sum((X[i] - self.X_train) ** 2, axis=1))
    return dists
```

**原理分析**：
- 外层循环遍历测试样本，内层使用向量化操作计算与所有训练样本的距离
- 利用NumPy的广播机制：`X[i]`的形状为`(D,)`，`self.X_train`的形状为`(N_train, D)`
- 广播后`(X[i] - self.X_train)`的形状为`(N_train, D)`，然后沿axis=1求和
- 时间复杂度：$O(N_{test} \times N_{train} \times D)$，但常数因子更小

**特点**：
- 相比双重循环有一定加速，但仍需要循环
- 代码可读性较好，是双重循环和完全向量化之间的折中

#### 2.2.3 完全向量化实现（compute_distances_no_loops）

**实现代码**：
```python
def compute_distances_no_loops(self, X):
    num_test = X.shape[0]
    num_train = self.X_train.shape[0]
    dists = np.zeros((num_test, num_train))
    
    # Using the formula: (a-b)^2 = a^2 - 2ab + b^2
    # dists[i, j] = sqrt(sum((X[i] - X_train[j])^2))
    # = sqrt(sum(X[i]^2) - 2*X[i]*X_train[j]^T + sum(X_train[j]^2))
    dists = np.sqrt(
        np.sum(X**2, axis=1, keepdims=True) + 
        np.sum(self.X_train**2, axis=1) - 
        2 * np.dot(X, self.X_train.T)
    )
    return dists
```

**原理分析**：
- 利用数学恒等式：$(a-b)^2 = a^2 - 2ab + b^2$
- 展开L2距离公式：
  $$d^2(x_i, x_j) = \sum_d (x_{i,d} - x_{j,d})^2 = \sum_d x_{i,d}^2 - 2\sum_d x_{i,d}x_{j,d} + \sum_d x_{j,d}^2$$
- 矩阵化表示：
  - `np.sum(X**2, axis=1, keepdims=True)`：形状`(N_test, 1)`，每行是测试样本的平方和
  - `np.sum(self.X_train**2, axis=1)`：形状`(N_train,)`，每个训练样本的平方和
  - `np.dot(X, self.X_train.T)`：形状`(N_test, N_train)`，所有点对的点积
  - 利用广播机制自动扩展维度进行相加
- 时间复杂度：$O(N_{test} \times N_{train} \times D)$，但充分利用了BLAS库的优化

**特点**：
- 完全向量化，无显式循环
- 充分利用NumPy和底层BLAS库的优化，速度最快
- 代码简洁，但需要理解矩阵运算和广播机制

### 2.3 标签预测实现（predict_labels）

**实现代码**：
```python
def predict_labels(self, dists, k=1):
    num_test = dists.shape[0]
    y_pred = np.zeros(num_test)
    for i in range(num_test):
        # 找到k个最近邻的索引
        closest_indices = np.argsort(dists[i])[:k]
        closest_y = self.y_train[closest_indices]
        
        # 投票选择最常见的标签
        counts = np.bincount(closest_y)
        y_pred[i] = np.argmax(counts)
    return y_pred
```

**原理分析**：
- `np.argsort(dists[i])`：对第i个测试样本到所有训练样本的距离排序，返回索引
- `[:k]`：取前k个最近邻的索引
- `np.bincount(closest_y)`：统计k个最近邻中每个标签出现的次数
- `np.argmax(counts)`：选择出现次数最多的标签（平局时选择较小的标签）

**特点**：
- 使用NumPy的高效排序和计数函数
- 时间复杂度：$O(N_{test} \times (N_{train} \log N_{train} + k))$

## 3 实验结果

### 3.1 性能对比

三种距离计算方法的执行时间对比如下：

| 实现方式 | 执行时间（秒） | 相对速度 |
|---------|--------------|---------|
| 双重循环（two_loops） | 12.18 | 1.0× |
| 单循环（one_loop） | 8.80 | 1.4× |
| 完全向量化（no_loops） | 0.10 | 116.5× |

**性能分析**：
- **双重循环 → 单循环**：速度提升约1.4倍，通过减少循环嵌套和利用向量化操作实现
- **单循环 → 完全向量化**：速度提升约88倍，通过消除所有循环和利用矩阵运算实现
- **双重循环 → 完全向量化**：总体速度提升约116.5倍，展现了向量化编程的巨大优势

**关键发现**：
1. NumPy的向量化操作充分利用了底层BLAS库的优化，能够并行计算
2. 矩阵乘法（`np.dot`）是高度优化的操作，比循环快得多
3. 广播机制使得代码既简洁又高效

### 3.2 分类准确率

#### 3.2.1 初步测试结果

在500个测试样本上的初步测试：

| k值 | 正确样本数 | 准确率 |
|-----|-----------|--------|
| k=1 | 137/500 | 27.4% |
| k=5 | 139/500 | 27.8% |

**分析**：
- k=1时，分类器选择最近的单个训练样本的标签
- k=5时，通过投票机制，准确率略有提升
- 准确率较低（约27-28%）是正常的，因为：
  - 使用原始像素值作为特征，信息量有限
  - CIFAR-10有10个类别，随机猜测的准确率只有10%
  - kNN是简单的非参数方法，没有学习特征表示

#### 3.2.2 交叉验证结果

使用5折交叉验证选择最优k值，候选k值：`[1, 3, 5, 8, 10, 12, 15, 20, 50, 100]`

**交叉验证准确率统计**：

| k值 | 各折准确率 | 平均准确率 | 标准差 |
|-----|-----------|-----------|--------|
| k=1 | 0.263, 0.257, 0.264, 0.278, 0.266 | 0.2656 | 0.0078 |
| k=3 | 0.239, 0.249, 0.240, 0.266, 0.254 | 0.2496 | 0.0104 |
| k=5 | 0.248, 0.266, 0.280, 0.292, 0.280 | 0.2732 | 0.0168 |
| k=8 | 0.262, 0.282, 0.273, 0.290, 0.273 | 0.2760 | 0.0098 |
| k=10 | 0.265, 0.296, 0.276, 0.284, 0.280 | **0.2802** | 0.0108 |
| k=12 | 0.260, 0.295, 0.279, 0.283, 0.280 | 0.2794 | 0.0124 |
| k=15 | 0.252, 0.289, 0.278, 0.282, 0.274 | 0.2750 | 0.0128 |
| k=20 | 0.270, 0.279, 0.279, 0.282, 0.285 | 0.2790 | 0.0054 |
| k=50 | 0.271, 0.288, 0.278, 0.269, 0.266 | 0.2744 | 0.0086 |
| k=100 | 0.256, 0.270, 0.263, 0.256, 0.263 | 0.2616 | 0.0054 |

**最优k值选择**：
- **最佳k值**：k=10，平均准确率0.2802（28.02%）
- **次优k值**：k=12，平均准确率0.2794（27.94%）

**k值选择分析**：
1. **k值过小（k=1, 3）**：
   - 对噪声敏感，容易过拟合
   - 决策边界复杂，泛化能力差
   - 准确率相对较低

2. **k值适中（k=5, 8, 10, 12）**：
   - 通过投票机制平滑决策边界
   - 对噪声更鲁棒
   - 准确率较高且稳定

3. **k值过大（k=50, 100）**：
   - 决策边界过于平滑，可能欠拟合
   - 忽略了局部特征
   - 准确率开始下降

#### 3.2.3 测试集最终结果

使用交叉验证选择的最优k=10在完整测试集上的表现：

- **测试样本数**：500
- **正确预测数**：141
- **准确率**：28.2%

**结果分析**：
- 测试集准确率（28.2%）与交叉验证平均准确率（28.02%）非常接近，说明：
  - 交叉验证有效估计了模型的泛化性能
  - 没有明显的过拟合或欠拟合
  - 模型选择是可靠的

- 相比随机猜测（10%），kNN取得了显著的提升
- 相比深度学习方法（通常>90%），kNN准确率较低，这是因为：
  - 使用原始像素特征，没有学习层次化特征表示
  - 非参数方法，无法学习数据的内在结构
  - 但kNN实现简单，无需训练过程

### 3.3 交叉验证可视化

交叉验证结果的折线图展示了不同k值下的准确率分布：

**关键观察**：
1. **准确率波动**：每个k值在5折上的准确率存在一定波动，反映了数据分割的随机性
2. **最优区间**：k值在8-12之间表现最好，准确率稳定在27-29%之间
3. **趋势分析**：
   - k值从1增加到10时，准确率总体上升
   - k值超过12后，准确率开始下降
   - 说明适中的k值能够平衡偏差和方差

## 4 算法实现细节分析

### 4.1 距离计算的数学原理

**L2距离的矩阵化推导**：

对于测试样本$x_i$和训练样本$x_j$，L2距离的平方为：
$$d^2(x_i, x_j) = \sum_{d=1}^{D}(x_{i,d} - x_{j,d})^2$$

展开平方项：
$$d^2(x_i, x_j) = \sum_{d=1}^{D}(x_{i,d}^2 - 2x_{i,d}x_{j,d} + x_{j,d}^2)$$

分离各项：
$$d^2(x_i, x_j) = \sum_{d=1}^{D}x_{i,d}^2 - 2\sum_{d=1}^{D}x_{i,d}x_{j,d} + \sum_{d=1}^{D}x_{j,d}^2$$

矩阵表示：
- $\sum_{d}x_{i,d}^2$：测试样本的平方和，形状`(N_test, 1)`
- $\sum_{d}x_{j,d}^2$：训练样本的平方和，形状`(N_train,)`
- $\sum_{d}x_{i,d}x_{j,d}$：点积，通过`np.dot(X, X_train.T)`计算，形状`(N_test, N_train)`

利用广播机制，三个矩阵自动扩展并相加，得到距离矩阵。

### 4.2 向量化的优势

**为什么向量化如此高效？**

1. **底层优化**：NumPy使用高度优化的BLAS（Basic Linear Algebra Subprograms）库
2. **并行计算**：矩阵运算可以并行执行，充分利用多核CPU
3. **缓存友好**：连续内存访问模式，缓存命中率高
4. **减少Python开销**：减少Python解释器的函数调用开销

**性能提升的量化分析**：
- 双重循环：每次迭代都需要Python解释器处理
- 单循环：部分向量化，减少了Python开销
- 完全向量化：几乎全部在C层执行，Python开销最小

### 4.3 预测算法的复杂度分析

**时间复杂度**：
- **距离计算**：$O(N_{test} \times N_{train} \times D)$
- **排序**：$O(N_{test} \times N_{train} \log N_{train})$
- **投票**：$O(N_{test} \times k)$
- **总体**：$O(N_{test} \times (N_{train} \times D + N_{train} \log N_{train} + k))$

**空间复杂度**：
- **距离矩阵**：$O(N_{test} \times N_{train})$
- 对于大规模数据，这是kNN的主要瓶颈

## 5 实验总结

### 5.1 主要成果

1. **成功实现kNN分类器**：
   - 完成了三种不同效率的距离计算方法
   - 实现了标签预测和投票机制
   - 在CIFAR-10数据集上达到28.2%的分类准确率

2. **验证了向量化的威力**：
   - 完全向量化实现比双重循环快116.5倍
   - 深刻理解了NumPy矩阵运算和广播机制
   - 掌握了高效数值计算的编程技巧

3. **掌握了交叉验证方法**：
   - 使用5折交叉验证选择最优超参数
   - 验证了模型选择的可靠性
   - 理解了偏差-方差权衡

### 5.2 关键发现

1. **k值的选择**：
   - 过小的k值（k=1）容易过拟合，对噪声敏感
   - 过大的k值（k>20）可能欠拟合，忽略局部特征
   - 适中的k值（k=8-12）能够平衡偏差和方差

2. **向量化的重要性**：
   - 在Python科学计算中，向量化是提高性能的关键
   - 理解矩阵运算的数学原理有助于写出高效的代码
   - 充分利用NumPy的优化能够获得数量级的性能提升

3. **kNN的局限性**：
   - 使用原始像素特征，准确率有限
   - 测试时需要计算与所有训练样本的距离，效率低
   - 对高维数据效果差（维度灾难）

### 5.3 与理论预期的对比

实验结果与kNN算法的理论特性高度一致：

1. **非参数学习**：✓ kNN不需要训练过程，只是记忆数据
2. **基于距离的预测**：✓ 通过L2距离找到最近邻
3. **投票机制**：✓ k>1时通过多数投票决定标签
4. **交叉验证有效性**：✓ 交叉验证准确估计了泛化性能

### 5.4 改进方向

虽然当前实现已经能够正常工作，但仍有改进空间：

1. **特征工程**：
   - 使用更好的特征（如HOG、颜色直方图）替代原始像素
   - 进行特征降维（PCA）减少计算量
   - 特征归一化提高距离度量的有效性

2. **算法优化**：
   - 使用KD树或Ball树加速最近邻搜索
   - 对于大规模数据，使用近似最近邻（ANN）算法
   - 实现加权kNN，根据距离给不同邻居不同权重

3. **距离度量**：
   - 尝试L1距离（曼哈顿距离）
   - 尝试余弦相似度
   - 学习距离度量（Metric Learning）

4. **数据预处理**：
   - 数据归一化（零均值、单位方差）
   - 数据增强提高泛化能力

## 6 结论

本实验成功实现了k-最近邻分类器，并通过三种不同效率的实现方式展示了向量化编程的重要性。实验结果表明：

1. **算法实现**：成功实现了kNN的核心功能，包括距离计算和标签预测
2. **性能优化**：完全向量化实现相比双重循环提升了116.5倍的速度
3. **超参数选择**：通过交叉验证确定了最优k=10，测试集准确率达到28.2%
4. **理论验证**：实验结果与kNN算法的理论特性一致

通过本实验，深入理解了：
- kNN算法的基本原理和实现细节
- NumPy向量化编程的重要性和技巧
- 交叉验证在模型选择中的作用
- 图像分类的基本流程和评估方法

虽然kNN在CIFAR-10上的准确率有限，但它作为最简单的分类算法之一，为理解更复杂的机器学习方法奠定了基础。未来可以通过特征工程、距离度量学习和近似算法等方式进一步提升kNN的性能和效率。
