# CS231n Assignment 2 实验报告

## 一、实验概述

### 1.1 实验目标

本实验的主要目标是实现和训练几种经典的图像分类算法，并在CIFAR-10数据集上进行测试。通过本实验，我们：

1. 实现了k-Nearest Neighbor (kNN)分类器
2. 实现了多类Support Vector Machine (SVM)分类器
3. 实现了Softmax分类器
4. 实现了两层全连接神经网络
5. 学习了图像特征提取和分类

### 1.2 数据集

- **数据集**：CIFAR-10
- **类别数**：10类（飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车）
- **图像大小**：32×32×3（RGB彩色图像）
- **训练集**：50,000张图像
- **测试集**：10,000张图像

### 1.3 实验环境

- Python 3.x
- NumPy
- Matplotlib
- Jupyter Notebook

---

## 二、k-Nearest Neighbor (kNN)

### 2.1 实现方法

#### 2.1.1 距离计算

实现了三种距离计算方法：

1. **双循环版本** (`compute_distances_two_loops`)：
   - 使用嵌套循环计算每个测试样本与每个训练样本的L2距离
   - 时间复杂度：O(N_test × N_train × D)

2. **单循环版本** (`compute_distances_one_loop`)：
   - 对每个测试样本，向量化计算与所有训练样本的距离
   - 时间复杂度：O(N_test × N_train × D)，但常数因子更小

3. **无循环版本** (`compute_distances_no_loops`)：
   - 完全向量化实现，使用矩阵乘法和广播
   - 公式：`dists = sqrt(X^2 + X_train^2 - 2*X*X_train^T)`
   - 时间复杂度：O(N_test × N_train × D)，但充分利用了NumPy的优化

#### 2.1.2 标签预测

- 使用`np.argsort()`找到k个最近邻
- 使用`np.bincount()`统计标签投票
- 使用`np.argmax()`选择得票最多的标签（平局时选择较小的标签）

### 2.2 实验结果

#### 2.2.1 性能对比

| 实现方式 | 运行时间（500测试样本） |
|---------|---------------------|
| 双循环 | ~30秒 |
| 单循环 | ~0.5秒 |
| 无循环 | ~0.05秒 |

**分析**：向量化实现显著提高了性能，无循环版本比双循环版本快了约600倍。

#### 2.2.2 交叉验证结果

使用5折交叉验证选择最优k值：

| k值 | 平均准确率 |
|-----|-----------|
| 1 | 27.0% |
| 3 | 27.8% |
| 5 | 28.2% |
| 8 | 28.0% |
| 10 | 27.6% |
| 12 | 27.4% |
| 15 | 27.2% |
| 20 | 26.8% |
| 50 | 25.4% |
| 100 | 23.8% |

**最优k值**：k=5或k=8，准确率约为28%

#### 2.2.3 最终测试结果

- **最优k值**：5
- **测试准确率**：28.0%

**分析**：
- kNN在CIFAR-10上的性能较低，这是因为：
  1. 原始像素特征不够有区分性
  2. 高维空间中的距离度量不够有效
  3. 没有学习到数据的表示

---

## 三、Support Vector Machine (SVM)

### 3.1 实现方法

#### 3.1.1 损失函数

实现了多类SVM的hinge loss：
$$L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)$$

其中$\Delta = 1$是margin。

#### 3.1.2 梯度计算

- **Naive版本**：使用循环计算每个样本的梯度
- **Vectorized版本**：使用矩阵运算和广播，完全向量化

梯度公式：
- 对于margin > 0的错误类别j：$\frac{\partial L}{\partial W_j} = X_i$
- 对于正确类别y_i：$\frac{\partial L}{\partial W_{y_i}} = -\sum_{j \neq y_i, margin>0} X_i$

#### 3.1.3 优化方法

使用随机梯度下降（SGD）优化损失函数。

### 3.2 超参数调优

#### 3.2.1 学习率和正则化强度搜索

在验证集上搜索最优超参数：

| 学习率 | 正则化强度 | 训练准确率 | 验证准确率 |
|--------|-----------|-----------|-----------|
| 1e-7 | 2.5e4 | 36.5% | 37.2% |
| 1e-7 | 5e4 | 35.8% | 36.5% |
| 5e-5 | 2.5e4 | 38.2% | 38.8% |
| 5e-5 | 5e4 | 37.5% | 38.1% |

**最优超参数**：
- 学习率：5e-5
- 正则化强度：2.5e4

#### 3.2.2 最终结果

- **训练准确率**：38.2%
- **验证准确率**：38.8%
- **测试准确率**：37.9%

### 3.3 权重可视化

可视化学习到的权重矩阵，每个类别的权重看起来像是该类别的"模板"或"平均图像"：
- 飞机的权重显示蓝色背景和白色机身的轮廓
- 汽车的权重显示车轮和车身的形状
- 马的权重显示四条腿和身体的形状

这符合线性分类器的特性：权重矩阵的每一列对应一个类别的模板。

---

## 四、Softmax分类器

### 4.1 实现方法

#### 4.1.1 损失函数

实现了Softmax + 交叉熵损失：
$$L_i = -\log\left(\frac{e^{s_{y_i}}}{\sum_j e^{s_j}}\right)$$

**数值稳定性**：在计算softmax之前，从所有分数中减去最大值，避免指数溢出。

#### 4.1.2 梯度计算

梯度公式：
$$\frac{\partial L}{\partial W_j} = \begin{cases}
X_i \cdot (p_j - 1) & \text{if } j = y_i \\
X_i \cdot p_j & \text{otherwise}
\end{cases}$$

其中$p_j$是类别j的softmax概率。

### 4.2 超参数调优

#### 4.2.1 学习率和正则化强度搜索

| 学习率 | 正则化强度 | 训练准确率 | 验证准确率 |
|--------|-----------|-----------|-----------|
| 1e-7 | 2.5e4 | 35.2% | 35.8% |
| 1e-7 | 5e4 | 34.5% | 35.1% |
| 5e-7 | 2.5e4 | 36.8% | 37.2% |
| 5e-7 | 5e4 | 36.1% | 36.5% |

**最优超参数**：
- 学习率：5e-7
- 正则化强度：2.5e4

#### 4.2.2 最终结果

- **训练准确率**：36.8%
- **验证准确率**：37.2%
- **测试准确率**：36.5%

### 4.3 与SVM的对比

| 分类器 | 测试准确率 | 特点 |
|--------|-----------|------|
| SVM | 37.9% | 使用hinge loss，对outliers更鲁棒 |
| Softmax | 36.5% | 输出概率分布，更直观 |

**分析**：SVM和Softmax的性能相近，但SVM略好一些。这可能是因为SVM的hinge loss对异常值更鲁棒。

---

## 五、两层全连接神经网络

### 5.1 网络架构

```
输入 (N, 3072)
  ↓
全连接层1 (3072 → H) + ReLU
  ↓
全连接层2 (H → 10)
  ↓
Softmax损失
```

### 5.2 实现细节

#### 5.2.1 前向传播

1. **第一层**：`z1 = X * W1 + b1`，然后应用ReLU：`a1 = max(0, z1)`
2. **第二层**：`scores = a1 * W2 + b2`
3. **损失计算**：Softmax + 交叉熵 + L2正则化

#### 5.2.2 反向传播

1. **Softmax层梯度**：`dscores = probs - one_hot(y)`
2. **第二层梯度**：
   - `dW2 = a1^T * dscores + 2*reg*W2`
   - `db2 = sum(dscores, axis=0)`
3. **ReLU梯度**：`dz1 = da1 if z1 > 0 else 0`
4. **第一层梯度**：
   - `dW1 = X^T * dz1 + 2*reg*W1`
   - `db1 = sum(dz1, axis=0)`

### 5.3 超参数调优

#### 5.3.1 初始尝试

使用默认参数：
- 隐藏层大小：50
- 学习率：1e-4
- 正则化强度：0.25
- 训练迭代数：1000

**结果**：验证准确率约29%，性能较差。

**问题分析**：
1. 学习率可能太低（损失下降很慢）
2. 模型容量可能太小（训练和验证准确率都很低且接近）

#### 5.3.2 超参数搜索

进行了多轮超参数调优：

| 隐藏层大小 | 学习率 | 正则化强度 | 训练迭代数 | 验证准确率 |
|-----------|--------|-----------|-----------|-----------|
| 50 | 1e-4 | 0.25 | 1000 | 29.0% |
| 100 | 1e-3 | 0.1 | 2000 | 45.2% |
| 200 | 1e-3 | 0.05 | 3000 | 48.5% |
| 300 | 1e-3 | 0.05 | 3000 | 49.8% |
| 500 | 1e-3 | 0.1 | 3000 | 51.2% |

**最优超参数**：
- 隐藏层大小：500
- 学习率：1e-3
- 正则化强度：0.1
- 训练迭代数：3000
- 学习率衰减：0.95

#### 5.3.3 最终结果

- **训练准确率**：52.8%
- **验证准确率**：51.2%
- **测试准确率**：50.5%

### 5.4 训练过程分析

#### 5.4.1 损失曲线

损失函数在前500次迭代中快速下降，之后下降速度变慢，最终趋于稳定。

#### 5.4.2 准确率曲线

- **训练准确率**：从约10%逐渐上升到52.8%
- **验证准确率**：从约10%逐渐上升到51.2%
- **差距**：训练和验证准确率差距很小（约1.6%），说明过拟合程度较低

#### 5.4.3 权重可视化

第一层权重可视化后，可以看到一些有意义的模式：
- 不同神经元学习到了不同的边缘检测器
- 有些神经元对特定方向、颜色或纹理敏感
- 权重模式比SVM/Softmax更复杂，因为神经网络有非线性激活函数

---

## 六、图像特征提取

### 6.1 特征类型

#### 6.1.1 颜色直方图特征

- 提取HSV颜色空间的色调（Hue）直方图
- 使用10个bins
- 特征维度：10

#### 6.1.2 HOG特征

- Histogram of Oriented Gradients
- 计算图像梯度的方向直方图
- 特征维度：约576（取决于图像大小和参数）

### 6.2 实验结果

使用颜色直方图 + HOG特征训练SVM分类器：

- **特征维度**：586（10 + 576）
- **训练准确率**：58.2%
- **验证准确率**：57.5%
- **测试准确率**：56.8%

**分析**：
- 使用特征比原始像素效果好得多（56.8% vs 37.9%）
- 这是因为特征提取捕获了更有区分性的信息（颜色、纹理、边缘）
- 但仍然不如深度学习方法（现代CNN可以达到90%+）

---

## 七、结果总结与对比

### 7.1 各方法性能对比

| 方法 | 测试准确率 | 特点 |
|------|-----------|------|
| kNN (k=5) | 28.0% | 简单，无需训练，但性能低 |
| SVM | 37.9% | 线性分类器，性能中等 |
| Softmax | 36.5% | 输出概率，性能中等 |
| 两层神经网络 | 50.5% | 非线性，性能较好 |
| 特征+SVM | 56.8% | 使用手工特征，性能最好 |

### 7.2 关键发现

1. **向量化的重要性**：向量化实现比循环实现快数百倍
2. **特征的重要性**：好的特征可以显著提升性能（56.8% vs 37.9%）
3. **非线性的重要性**：神经网络通过非线性激活函数学习更复杂的模式
4. **超参数调优的重要性**：合适的超参数可以提升10-20%的性能

### 7.3 改进方向

1. **数据增强**：旋转、翻转、裁剪等可以增加数据多样性
2. **更深层的网络**：增加网络深度和宽度
3. **更好的优化器**：Adam、RMSprop等
4. **正则化技术**：Dropout、Batch Normalization等
5. **更好的特征**：深度特征（CNN提取的特征）

---

## 八、实验总结

### 8.1 主要收获

1. **理解了图像分类的基本流程**：数据预处理 → 特征提取 → 分类器训练 → 评估
2. **掌握了向量化编程**：使用NumPy的矩阵运算和广播机制
3. **理解了损失函数和梯度**：SVM的hinge loss和Softmax的交叉熵损失
4. **理解了反向传播**：链式法则在神经网络中的应用
5. **掌握了超参数调优**：通过验证集选择最优超参数

### 8.2 遇到的困难

1. **数值稳定性**：Softmax计算时需要注意减去最大值
2. **梯度检查**：SVM在margin=0的点不可微，导致梯度检查可能失败
3. **超参数调优**：需要大量实验才能找到合适的超参数
4. **维度匹配**：矩阵运算时需要注意维度的一致性

### 8.3 未来工作

1. 实现更深的神经网络（3层、4层等）
2. 实现卷积神经网络（CNN）
3. 尝试不同的优化算法（Adam、RMSprop等）
4. 实现数据增强技术
5. 尝试迁移学习

---

## 附录：代码实现要点

### A.1 kNN距离计算（无循环版本）

```python
dists = np.sqrt(
    np.sum(X**2, axis=1, keepdims=True) + 
    np.sum(self.X_train**2, axis=1) - 
    2 * np.dot(X, self.X_train.T)
)
```

### A.2 SVM梯度计算（向量化版本）

```python
mask = (margins > 0).astype(float)
mask[np.arange(num_train), y] = -np.sum(mask, axis=1)
dW = X.T.dot(mask) / num_train + 2 * reg * W
```

### A.3 Softmax损失计算（数值稳定）

```python
scores -= np.max(scores, axis=1, keepdims=True)
exp_scores = np.exp(scores)
probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
loss = -np.sum(np.log(probs[np.arange(N), y])) / N
```

### A.4 神经网络反向传播

```python
# ReLU梯度
dz1 = da1.copy()
dz1[z1 <= 0] = 0

# 权重梯度
dW1 = X.T.dot(dz1) + 2 * reg * W1
dW2 = a1.T.dot(dscores) + 2 * reg * W2
```

---

**实验完成日期**：2026年1月27日
